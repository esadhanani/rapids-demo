{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUnIdSGVq91T",
        "outputId": "14507357-ebe8-44a4-e16b-8bae83a2dc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: 0.28s | GPU: 4.08s | Speedup ≈ 0.1×\n"
          ]
        }
      ],
      "source": [
        "# RAPIDS demo: GPU vs CPU regression speed\n",
        "!pip install cudf-cu12 cuml-cu12 --extra-index-url=https://pypi.nvidia.com -q\n",
        "\n",
        "import pandas as pd, numpy as np, time\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from cuml.linear_model import LinearRegression as cuLR\n",
        "import cudf\n",
        "\n",
        "N = 3_000_000\n",
        "X_cpu = pd.DataFrame(np.random.rand(N,3), columns=['a','b','c'])\n",
        "y_cpu = X_cpu['a']*3.5 + X_cpu['b']*2.1 + np.random.rand(N)\n",
        "\n",
        "# CPU\n",
        "t0=time.time()\n",
        "LinearRegression().fit(X_cpu,y_cpu)\n",
        "cpu=time.time()-t0\n",
        "\n",
        "# GPU\n",
        "X_gpu = cudf.DataFrame.from_pandas(X_cpu)\n",
        "y_gpu = cudf.Series(y_cpu)\n",
        "t1=time.time()\n",
        "cuLR().fit(X_gpu,y_gpu)\n",
        "gpu=time.time()-t1\n",
        "\n",
        "print(f\"CPU: {cpu:.2f}s | GPU: {gpu:.2f}s | Speedup ≈ {cpu/gpu:.1f}×\")"
      ]
    }
  ]
}